{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pycrfsuite\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "random_seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    sentences = []\n",
    "    with open(filename, encoding='iso8859-15') as f:\n",
    "        sent = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if (len(line) == 0 ):\n",
    "                if len(sent) != 0:\n",
    "                    sentences.append(sent)\n",
    "                    sent = []\n",
    "            else:\n",
    "                ls = line.split(' ')\n",
    "                word, tag = ls[0],ls[-1]\n",
    "                sent.append((word,tag))\n",
    "    return sentences\n",
    "\n",
    "def writeData(filename,sentences):\n",
    "    with open(filename,\"w\") as f:\n",
    "        for sent in sentences:\n",
    "            for tuple in sent:\n",
    "                f.write(tuple[0] + \" \"+ tuple[1]+\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "def writeData1(filename,sentences):\n",
    "    with open(filename,\"w\") as f:\n",
    "        for sent in sentences:\n",
    "            for tuple in sent:\n",
    "                f.write(tuple[0] + \"\\n\")\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag_sentence(sentences):\n",
    "    pos_tagged_sentences = []\n",
    "    for i, sent in enumerate(sentences):\n",
    "        # Obtain the list of tokens in the document\n",
    "        tokens = [token for token, label in sent]\n",
    "        # Perform POS tagging\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        # Take the word, POS tag, and its label\n",
    "        pos_tagged_sentences.append([(w, pos, label) for (w, label), (word, pos) in zip(sent, tagged)])\n",
    "    return pos_tagged_sentences\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    features = [\n",
    "        'bias',\n",
    "        'word.lower=' + word.lower(),\n",
    "        'word[-3:]=' + word[-3:],\n",
    "        'word[-2:]=' + word[-2:],\n",
    "        'word.isupper=%s' % word.isupper(),\n",
    "        'word.istitle=%s' % word.istitle(),\n",
    "        'word.isdigit=%s' % word.isdigit(),\n",
    "        'postag=' + postag,\n",
    "        'postag[:2]=' + postag[:2],\n",
    "    ]\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.extend([\n",
    "            '-1:word.lower=' + word1.lower(),\n",
    "            '-1:word.istitle=%s' % word1.istitle(),\n",
    "            '-1:word.isupper=%s' % word1.isupper(),\n",
    "            '-1:postag=' + postag1,\n",
    "            '-1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('BOS')\n",
    "        \n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.extend([\n",
    "            '+1:word.lower=' + word1.lower(),\n",
    "            '+1:word.istitle=%s' % word1.istitle(),\n",
    "            '+1:word.isupper=%s' % word1.isupper(),\n",
    "            '+1:postag=' + postag1,\n",
    "            '+1:postag[:2]=' + postag1[:2],\n",
    "        ])\n",
    "    else:\n",
    "        features.append('EOS')\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = loadData('ner.txt')\n",
    "sentences = pos_tag_sentence(sentences)\n",
    "X = [sent2features(s) for s in sentences]\n",
    "y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "# X = sentences\n",
    "# y=sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_seed)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.10, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bias', 'word.lower=studies', 'word[-3:]=ies', 'word[-2:]=es', 'word.isupper=False', 'word.istitle=True', 'word.isdigit=False', 'postag=NNS', 'postag[:2]=NN', 'BOS', '+1:word.lower=on', '+1:word.istitle=False', '+1:word.isupper=False', '+1:postag=IN', '+1:postag[:2]=IN'], ['bias', 'word.lower=on', 'word[-3:]=on', 'word[-2:]=on', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'postag=IN', 'postag[:2]=IN', '-1:word.lower=studies', '-1:word.istitle=True', '-1:word.isupper=False', '-1:postag=NNS', '-1:postag[:2]=NN', '+1:word.lower=the', '+1:word.istitle=False', '+1:word.isupper=False', '+1:postag=DT', '+1:postag[:2]=DT'], ['bias', 'word.lower=the', 'word[-3:]=the', 'word[-2:]=he', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'postag=DT', 'postag[:2]=DT', '-1:word.lower=on', '-1:word.istitle=False', '-1:word.isupper=False', '-1:postag=IN', '-1:postag[:2]=IN', '+1:word.lower=radioimmunoassay', '+1:word.istitle=False', '+1:word.isupper=False', '+1:postag=NN', '+1:postag[:2]=NN'], ['bias', 'word.lower=radioimmunoassay', 'word[-3:]=say', 'word[-2:]=ay', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'postag=NN', 'postag[:2]=NN', '-1:word.lower=the', '-1:word.istitle=False', '-1:word.isupper=False', '-1:postag=DT', '-1:postag[:2]=DT', '+1:word.lower=of', '+1:word.istitle=False', '+1:word.isupper=False', '+1:postag=IN', '+1:postag[:2]=IN'], ['bias', 'word.lower=of', 'word[-3:]=of', 'word[-2:]=of', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'postag=IN', 'postag[:2]=IN', '-1:word.lower=radioimmunoassay', '-1:word.istitle=False', '-1:word.isupper=False', '-1:postag=NN', '-1:postag[:2]=NN', '+1:word.lower=insulin', '+1:word.istitle=False', '+1:word.isupper=False', '+1:postag=NN', '+1:postag[:2]=NN'], ['bias', 'word.lower=insulin', 'word[-3:]=lin', 'word[-2:]=in', 'word.isupper=False', 'word.istitle=False', 'word.isdigit=False', 'postag=NN', 'postag[:2]=NN', '-1:word.lower=of', '-1:word.istitle=False', '-1:word.isupper=False', '-1:postag=IN', '-1:postag[:2]=IN', 'EOS']]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "# writeData(\"ner.train\",X_train)\n",
    "# writeData(\"ner.dev\",X_dev)\n",
    "# writeData(\"ner.test\",X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "for xseq, yseq in zip(X_train, y_train):\n",
    "    trainer.append(xseq, yseq)\n",
    "trainer.set_params({\n",
    "    'c1': 1.0,   # coefficient for L1 penalty\n",
    "    'c2': 1e-3,  # coefficient for L2 penalty\n",
    "    'max_iterations': 1000,  # stop earlier\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "trainer.train('ner.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "969 {'num': 969, 'scores': {}, 'loss': 5206.987555, 'feature_norm': 52.912217, 'error_norm': 17.438782, 'active_features': 1860, 'linesearch_trials': 2, 'linesearch_step': 0.5, 'time': 0.067}\n"
     ]
    }
   ],
   "source": [
    "trainer.params()\n",
    "trainer.logparser.last_iteration\n",
    "print(len(trainer.logparser.iterations), trainer.logparser.iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on Dev/Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Dev Set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.93      0.97      0.95      4593\n",
      "          D       0.83      0.69      0.75       384\n",
      "          T       0.47      0.32      0.38       302\n",
      "\n",
      "avg / total       0.90      0.91      0.90      5279\n",
      "\n",
      "Evaluating on Train Set\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          O       0.94      0.97      0.96     10874\n",
      "          D       0.80      0.65      0.71      1007\n",
      "          T       0.75      0.58      0.66       796\n",
      "\n",
      "avg / total       0.92      0.92      0.92     12677\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('ner.model')\n",
    "\n",
    "# Create a mapping of labels to indices\n",
    "labels = {\"D\": 1, \"T\": 2, \"O\":0}\n",
    "\n",
    "print(\"Evaluating on Dev Set\")\n",
    "#Evaluating on Dev Set\n",
    "y_pred_dev = [tagger.tag(xseq) for xseq in X_dev]\n",
    "# for i in range(len(X_test)):\n",
    "#     for x, y,yp in zip([x[1].split(\"=\")[1] for x in X_test[i]],y_test[i],y_pred_test[i]):\n",
    "#         print(\"%s (%s) (%s)\" % (x, y, yp))\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred_dev for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_dev for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    labels = [0,1,2],\n",
    "    target_names=[\"O\", \"D\",\"T\"]))\n",
    "\n",
    "\n",
    "print(\"Evaluating on Train Set\")\n",
    "#Evaluating on Train Set\n",
    "y_pred_test = [tagger.tag(xseq) for xseq in X_test]\n",
    "\n",
    "# Convert the sequences of tags into a 1-dimensional array\n",
    "predictions = np.array([labels[tag] for row in y_pred_test for tag in row])\n",
    "truths = np.array([labels[tag] for row in y_test for tag in row])\n",
    "\n",
    "# Print out the classification report\n",
    "print(classification_report(\n",
    "    truths, predictions,\n",
    "    labels = [0,1,2],\n",
    "    target_names=[\"O\", \"D\",\"T\"]))\n",
    "\n",
    "# for i in range(len(X_dev)):\n",
    "#     for x, y,yp in zip([x[1].split(\"=\")[1] for x in X_dev[i]],y_dev[i],y_pred_dev[i]):\n",
    "#         print(\"%s (%s) (%s)\" % (x, y, yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "D      -> D       2.735692\n",
      "O      -> O       2.035593\n",
      "T      -> T       1.833928\n",
      "D      -> O       -1.411420\n",
      "T      -> O       -1.456108\n",
      "O      -> D       -1.473607\n",
      "O      -> T       -2.462634\n",
      "D      -> T       -2.854172\n",
      "T      -> D       -4.945421\n",
      "\n",
      "Top unlikely transitions:\n",
      "D      -> D       2.735692\n",
      "O      -> O       2.035593\n",
      "T      -> T       1.833928\n",
      "D      -> O       -1.411420\n",
      "T      -> O       -1.456108\n",
      "O      -> D       -1.473607\n",
      "O      -> T       -2.462634\n",
      "D      -> T       -2.854172\n",
      "T      -> D       -4.945421\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "info = tagger.info()\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common(9))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(info.transitions).most_common()[-9:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "6.404469 D      word.lower=diabetes\n",
      "6.397222 D      word.lower=incontinence\n",
      "6.053253 D      word.lower=tumors\n",
      "5.649193 D      word.lower=cancers\n",
      "5.511970 D      -1:word.lower=baclofen\n",
      "5.439216 T      word.lower=fenfluramines\n",
      "5.116713 T      word.lower=resection\n",
      "4.974191 T      word.lower=antibiotics\n",
      "4.907409 T      word.lower=vaccination\n",
      "4.814898 T      word[-3:]=xel\n",
      "4.740379 D      word.lower=bleeding\n",
      "4.680517 D      word.lower=depression\n",
      "4.622520 T      word.lower=alteplase\n",
      "4.600431 T      +1:word.lower=yag\n",
      "4.461216 O      word.lower=versus\n",
      "4.392960 D      word[-2:]=lc\n",
      "4.378241 D      word.lower=infection\n",
      "4.340741 T      word.lower=ventilation\n",
      "4.239456 D      word.lower=strokes\n",
      "4.198719 D      word.lower=hypertension\n",
      "\n",
      "Top negative:\n",
      "-1.693500 O      postag[:2]=NN\n",
      "-1.723514 T      word[-2:]=as\n",
      "-1.790795 O      word[-3:]=oot\n",
      "-1.820180 O      word[-3:]=che\n",
      "-1.829894 D      +1:word.lower=for\n",
      "-1.832364 O      word.lower=lung\n",
      "-1.846011 O      word.lower=pulmonary\n",
      "-1.860870 O      word[-3:]=ias\n",
      "-1.944537 O      +1:word.lower=seven\n",
      "-1.954943 O      postag=JJ\n",
      "-2.019139 O      word[-3:]=rve\n",
      "-2.123087 O      word[-3:]=ils\n",
      "-2.158549 O      -1:word.lower=whom\n",
      "-2.211868 D      -1:word.lower=shock\n",
      "-2.280869 O      +1:word.lower=treatment\n",
      "-2.306956 D      word[-2:]=ls\n",
      "-2.338661 D      word[-3:]=sma\n",
      "-2.355047 O      +1:word.lower=bladder\n",
      "-2.358923 O      word[-3:]=omy\n",
      "-2.551600 O      +1:word.lower=prior\n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
